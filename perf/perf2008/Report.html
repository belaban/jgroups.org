<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML>
<HEAD>
	<META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset=utf-8">
	<TITLE>Performance tests JGroups</TITLE>
	<META NAME="GENERATOR" CONTENT="OpenOffice.org 3.0 Beta (Unix)">
	<META NAME="CREATED" CONTENT="0;0">
	<META NAME="CHANGED" CONTENT="20080815;15102800">
</HEAD>
<BODY LANG="en-US" DIR="LTR">
<H1>Performance tests JGroups 2.6.4</H1>
<P><BR>Author: Bela Ban<BR>Date: August 2008</P>
<H2>Configuration</H2>
<P><BR>The tests were conducted in the JBoss lab in Atlanta. Each box
was:</P>
<UL>
	<LI><P STYLE="margin-bottom: 0in">Dell PE1850 2x 3Ghz EM64T
	processors</P>
	<LI><P STYLE="margin-bottom: 0in">4GB memory 
	</P>
	<LI><P STYLE="margin-bottom: 0in">Red Hat Enterprise Linux (RHEL) 4</P>
	<LI><P>OS: Linux 2.6.9-42.0.10.smp</P>
</UL>
<P LANG="en"><BR>The tests were run only on 64-bit systems.</P>
<H3 LANG="en">Switches</H3>
<P LANG="en">The following switch was used:</P>
<UL>
	<LI><P><SPAN LANG="en">1GBps: <A HREF="http://docs.us.dell.com/support/edocs/network/5P788/en/ug/pc5224c1.htm">Dell
	Power Connect 5224</A> (24 port gige L2 managed switch) with jumbo
	frames enabled. All NICs had MTUs of 1500 bytes. We didn't have the
	time to switch to jumbo frames.</SPAN></P>
</UL>
<H3 LANG="en">Network</H3>
<P LANG="en">Note that neither the network nor the test machines were
reserved exclusively for the performance tests, so some of<BR>the
fluctuations are explained by the fact that other users/processes
were using the machines and/or generating traffic on the network.
This was mitigated somewhat by running all tests in the AM hours (8am
- 3pm) in Europe, when the servers in<BR>Atlanta GA were mostly idle.
We also used machine which have no cron jobs (like cruisecontrol etc)
scheduled that could interfere with the tests.<BR><BR><BR>
</P>
<H3 LANG="en">JVM configuration</H3>
<P LANG="en">The following JVMs were used:</P>
<UL>
	<LI><P LANG="en">SUN JDK 1.6.0_03-b05 
	</P>
</UL>
<P LANG="en">The options used for starting the JVMs were:<BR>-server
-Xmx600M -Xms400M -XX:+UseParallelGC -XX:+AggressiveHeap
-XX:CompileThreshold=100 -XX:SurvivorRatio=8
-XX:TargetSurvivorRatio=90 -XX:MaxTenuringThreshold=31
-Dlog4j.configuration=file:/home/bela/log4j.properties
-Djgroups.bind_addr=${MYTESTIP_1} -Dcom.sun.management.jmxremote
-Dresolve.dns=false<BR><BR><BR>
</P>
<H2 LANG="en">Design of the tests</H2>
<P LANG="en">The goal of the test is to measure the time it takes to
reliably send N messages to all nodes in a cluster. (We are currently
not interested in measuring remote group RPC, this may be done in a
future performance test.) The test works as follows. 
</P>
<UL>
	<LI><P LANG="en" STYLE="margin-bottom: 0in">All members have access
	to 2 configuration files: one for defining the number of messages to
	be sent, the number of cluster members, senders etc, and the other
	one defining the JGroups protocol stack. 
	</P>
	<LI><P LANG="en" STYLE="margin-bottom: 0in">N processes are started.
	They can be started on the same machine, but for better results
	there is one JGroups process per box 
	</P>
	<LI><P LANG="en" STYLE="margin-bottom: 0in">The processes join the
	same group, and when N equals the cluster size defined in the
	configuration file, all senders start sending M messages to the
	cluster (M is also defined in the configuration file). 
	</P>
	<LI><P LANG="en">Every node in the cluster receives the messages
	(senders are also receivers). When the first message is received,
	the timer is started. When the last message is received the timer is
	stopped. Every member knows how many messages are to be received:
	&lt;number of senders&gt; * &lt;number of messages&gt;. This allows
	every receiver to compute the message rate and throughput (we also
	know the size of each message). 
	</P>
</UL>
<P><SPAN LANG="en">The test is part of JGroups:
org.jgroups.tests.perf.Test. The driver is JGroups independent and
can also be used to measure JMS performance and pure UDP or TCP
performance (all that needs to be done is to write a Transport
interface implementation, with a send() and a receive()).<BR><BR>The
configuration file (config.txt) is shown below (only the JGroups
configuration is shown):<BR></SPAN><SAMP><CODE><FONT SIZE=1><SPAN LANG="en"><BR></SPAN></FONT></CODE></SAMP><SAMP><CODE><FONT SIZE=2><SPAN LANG="en">#
Class implementing the org.jgroups.tests.perf.Transport
interface<BR><BR>transport=org.jgroups.tests.perf.transports.JGroupsTransport<BR>#transport=org.jgroups.tests.perf.transports.UdpTransport<BR>#transport=org.jgroups.tests.perf.transports.TcpTransport<BR><BR>#
Number of messages a sender sendes<BR>num_msgs=1000000<BR><BR>#
Message size in bytes<BR>msg_size=1000<BR><BR># Expected number of
group members<BR>num_members=2<BR><BR># Number of senders in the
group. Min 1, max num_members<BR>num_senders=2<BR><BR># dump stats
every n msgs<BR>log_interval=100000</SPAN></FONT></CODE></SAMP><SAMP><CODE><FONT SIZE=1><SPAN LANG="en"><BR></SPAN></FONT></CODE></SAMP><SPAN LANG="en"><BR><BR>This
file must be the same for all nodes, so it is suggested to place it
in a shared file system, e.g. an NFS mounted directory.<BR>The
following parameters are used:</SPAN></P>
<TABLE WIDTH=1094 BORDER=1 CELLPADDING=2 CELLSPACING=0>
	<COL WIDTH=131>
	<COL WIDTH=953>
	<TR>
		<TD WIDTH=131>
			<P><B>transport</B></P>
		</TD>
		<TD WIDTH=953>
			<P>An implementation of Transport</P>
		</TD>
	</TR>
	<TR>
		<TD WIDTH=131>
			<P><B>num_msgs</B></P>
		</TD>
		<TD WIDTH=953>
			<P>Number of messages to be sent by a sender</P>
		</TD>
	</TR>
	<TR>
		<TD WIDTH=131>
			<P><B>msg_size</B></P>
		</TD>
		<TD WIDTH=953>
			<P>Number of bytes of a single message</P>
		</TD>
	</TR>
	<TR>
		<TD WIDTH=131>
			<P><B>num_members</B></P>
		</TD>
		<TD WIDTH=953>
			<P>Number of members. When members are started, they wait until
			num_members nodes have joined the cluster, and then the senders
			start sending messages</P>
		</TD>
	</TR>
	<TR>
		<TD WIDTH=131>
			<P><B>num_senders</B></P>
		</TD>
		<TD WIDTH=953>
			<P>Number of senders (must be less then or equal to num_members).
			This allows each receiver to compute the total number of messages
			to be received: num_senders * num_msgs. In the example, we'll
			receive 2 million 1K messages</P>
		</TD>
	</TR>
	<TR>
		<TD WIDTH=131>
			<P><B>log_interval</B></P>
		</TD>
		<TD WIDTH=953>
			<P>Output about sent and received messages will be output to
			stdout (and to file if -f is used (see below)) every log_interval
			messages</P>
		</TD>
	</TR>
</TABLE>
<P><SPAN LANG="en"><BR>The options for the test driver
are:<BR></SPAN><SAMP><SPAN LANG="en">bela@laptop /cygdrive/c<BR>$
java org.jgroups.tests.perf.Test -help<BR>Test [-help] ([-sender] |
[-receiver]) [-config &lt;config file&gt;] [-props &lt;stack config&gt;]
[-verbose] [-jmx] [-dump_stats] [-f &lt;filename&gt;]</SPAN></SAMP></P>
<TABLE BORDER=1 CELLPADDING=2 CELLSPACING=2>
	<TR>
		<TD>
			<P><B>-sender / - receiver</B></P>
		</TD>
		<TD>
			<P>Whether this process is a sender or a receiver (a sender is
			always a receiver as well)</P>
		</TD>
	</TR>
	<TR>
		<TD>
			<P><B>-config &lt;file&gt;</B></P>
		</TD>
		<TD>
			<P>Points to the configuration file, e.g. -config
			/home/bela/config.txt</P>
		</TD>
	</TR>
	<TR>
		<TD>
			<P><B>-props &lt;props&gt;</B></P>
		</TD>
		<TD>
			<P>The JGroups protocol stack configuration. Example: -props
			c:\fc-fast-minimalthreads.xml. Can be any URL</P>
		</TD>
	</TR>
	<TR>
		<TD>
			<P><B>-verbose</B></P>
		</TD>
		<TD>
			<P>Verbose output</P>
		</TD>
	</TR>
	<TR>
		<TD>
			<P><B>-jmx</B></P>
		</TD>
		<TD>
			<P>Enables JMX instrumentation (requires JVM with JMX MBeanServer,
			e.g. JDK5). This will cause the VM not to terminate when done. To
			access the process via JMX, the -Dcom.sun.management.jmxremote
			property has to be defined and jconsole can be used. For more
			details see <A HREF="http://wiki.jboss.org/wiki/Wiki.jsp?page=JMX">http://wiki.jboss.org/wiki/Wiki.jsp?page=JMX</A>.</P>
		</TD>
	</TR>
	<TR>
		<TD>
			<P><B>-dump_stats</B></P>
		</TD>
		<TD>
			<P>Dumps some JMX statistics after the run, e.g. number of
			messages sent, number of times blocked etc</P>
		</TD>
	</TR>
	<TR>
		<TD>
			<P><B>-f &lt;filename&gt;</B></P>
		</TD>
		<TD>
			<P>Dumps number of messages sent, message rate, throughput,
			current time, free and total memory to &lt;filename&gt; every
			log_interval milliseonds. This is the main tool to generate charts
			on memory behavior or message rate variance</P>
		</TD>
	</TR>
	<TR>
		<TD>
			<P><B>-help</B></P>
		</TD>
		<TD>
			<P>This help</P>
		</TD>
	</TR>
</TABLE>
<P><SPAN LANG="en"><BR>The files needed to run te tests are included
with JGroups (source distribution) as well: JGroups/conf/config.txt
is the test configuration file, and JGroups/conf/{udp,tcp}.xml are
the JGroups protocol stack config files. Note that all the test runs
will have the JGroups stack configuration file included so that they
can be reproduced.<BR><BR>For additional details on how to run the
tests refer to <A HREF="http://wiki.jboss.org/wiki/Wiki.jsp?page=PerfTests">http://wiki.jboss.org/wiki/Wiki.jsp?page=PerfTests</A>.
<BR></SPAN><BR><BR>
</P>
<H2 LANG="en">Parameters</H2>
<P LANG="en">The parameters that are varied are:</P>
<UL>
	<LI><P LANG="en" STYLE="margin-bottom: 0in">Cluster size: 4, 6, 8
	and 10</P>
	<LI><P LANG="en" STYLE="margin-bottom: 0in">Number of senders: N
	(where N is the cluster size); every node in the cluster is a sender</P>
	<LI><P LANG="en" STYLE="margin-bottom: 0in">Number of messages: 1
	million (constant, regardless of cluster size)</P>
	<LI><P LANG="en" STYLE="margin-bottom: 0in">Message size: 1K, 2.5K
	and 5K</P>
	<LI><P LANG="en" STYLE="margin-bottom: 0in">Switch: 1GB/sec 
	</P>
	<LI><P LANG="en" STYLE="margin-bottom: 0in">Protocols: IP
	multicast-based transport (udp.xml) and TCP-based transport
	(tcp.xml)</P>
	<LI><P LANG="en" STYLE="margin-bottom: 0in">JVMs: SUN JDK6</P>
	<LI><P LANG="en">JGroups 2.6.4.RC1 (CVS Aug 15 2008)</P>
</UL>
<P><BR><BR>
</P>
<H2 LANG="en">Results</H2>
<P><SPAN LANG="en">The plain results are available <A HREF="4">here
</A>for the 4 node cluster, <A HREF="6">here </A>for the 6 node
cluster and <A HREF="8">here </A>for the 8 node cluster. The
OpenOffice spreadsheet containing all results can be obtained <A HREF="charts/perf.ods">here</A>.
<BR>The performance tests were run on the cluster, measuring message
rate (messages received/sec) and throughput (MB received/sec), as
described in <A HREF="http://wiki.jboss.org/wiki/Wiki.jsp?page=PerfTests">http://wiki.jboss.org/wiki/Wiki.jsp?page=PerfTests</A>.<BR>The
JGroups configurations used are:</SPAN></P>
<UL>
	<LI><P STYLE="margin-bottom: 0in"><SPAN LANG="en">UDP (<A HREF="udp.xml">udp.xml</A>):
	uses IP multicast in the transport, with FC as flow control
	implementation</SPAN></P>
	<LI><P><SPAN LANG="en">TCP (<A HREF="tcp.xml">tcp.xml</A>): uses
	multiple TCP connections in the transport </SPAN>
	</P>
</UL>
<P><BR><BR>
</P>
<H3 LANG="en">Message rate and throughput</H3>
<P LANG="en">The Y axes define message rate and throughput, and the X
axis defines the message size. The message rate is computed as
&lt;number of messages sent&gt; * &lt;number of senders&gt; / &lt;time
to receive all messages&gt;. For example, a message rate of 35000 for
1K means that a receiver received 35000 1K messages on average per
second. On average means that - at the end of a test run - all nodes
post their results, which are collected, summed up and divided by the
number of nodes. So if we have 2 nodes, and they post message rates
of 10000 and 20000, the average messsage rate will be 15000.<BR>The
throughput is simply the average message rate times the message
size.<BR><BR><BR>
</P>
<H3 LANG="en">4 node cluster</H3>
<TABLE BORDER=0 CELLPADDING=2 CELLSPACING=2>
	<TR>
		<TD>
			<P><IMG SRC="charts/4_rate.png" NAME="graphics1" ALIGN=BOTTOM WIDTH=557 HEIGHT=376 BORDER=0></P>
		</TD>
		<TD>
			<P><IMG SRC="charts/4_throughput.png" NAME="graphics2" ALIGN=BOTTOM WIDTH=568 HEIGHT=375 BORDER=0></P>
		</TD>
	</TR>
</TABLE>
<P ALIGN=CENTER STYLE="margin-bottom: 0in"><BR>
</P>
<P><BR><BR>
</P>
<H3 LANG="en">6 node cluster</H3>
<TABLE BORDER=0 CELLPADDING=2 CELLSPACING=2>
	<TR>
		<TD>
			<P><IMG SRC="charts/6_rate.png" NAME="graphics3" ALIGN=BOTTOM WIDTH=536 HEIGHT=373 BORDER=0></P>
		</TD>
		<TD>
			<P><IMG SRC="charts/6_throughput.png" NAME="graphics4" ALIGN=BOTTOM WIDTH=536 HEIGHT=373 BORDER=0></P>
		</TD>
	</TR>
</TABLE>
<P LANG="en"><BR><BR><BR>
</P>
<H3 LANG="en">8 node cluster</H3>
<TABLE BORDER=0 CELLPADDING=2 CELLSPACING=2>
	<TR>
		<TD>
			<P><IMG SRC="charts/8_rate.png" NAME="graphics5" ALIGN=BOTTOM WIDTH=536 HEIGHT=373 BORDER=0></P>
		</TD>
		<TD>
			<P><IMG SRC="charts/8_throughput.png" NAME="graphics6" ALIGN=BOTTOM WIDTH=536 HEIGHT=373 BORDER=0></P>
		</TD>
	</TR>
</TABLE>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<H3>10 node cluster</H3>
<P><IMG SRC="charts/10_rate.png" NAME="graphics7" ALIGN=BOTTOM WIDTH=536 HEIGHT=373 BORDER=0><IMG SRC="charts/10_throughput.png" NAME="graphics8" ALIGN=BOTTOM WIDTH=536 HEIGHT=373 BORDER=0><SPAN LANG="en"><BR><BR></SPAN><BR><BR>
</P>
<H3>Scalability per node</H3>
<P><IMG SRC="charts/rates_node.png" NAME="graphics9" ALIGN=BOTTOM WIDTH=536 HEIGHT=373 BORDER=0><IMG SRC="charts/throughput_node.png" NAME="graphics10" ALIGN=BOTTOM WIDTH=536 HEIGHT=373 BORDER=0></P>
<H3>Scalability per cluster</H3>
<P><IMG SRC="charts/rates_cluster.png" NAME="graphics11" ALIGN=BOTTOM WIDTH=536 HEIGHT=373 BORDER=0><IMG SRC="charts/throughput_cluster.png" NAME="graphics12" ALIGN=BOTTOM WIDTH=536 HEIGHT=373 BORDER=0></P>
<H2 LANG="en">Interpretation of results</H2>
<H3 LANG="en">General</H3>
<P><SPAN LANG="en">All of the tests were conducted with a relatively
conservative configuration, which would allow for the tests to be run
for an extended period of time. For example, the number of credits in
flow control was relatively small. Had we set them to a higher value,
we would have probably gotten better results, but the tests would not
have been able to run for an extended period of time without running
out of memory. For instance, the number of credits should be a
function of the group size (more credits for more nodes in the
cluster), however, we used (more or less) the same configuration for
all tests. We will introduce an enhanced version of the flow control
protocol which dynamically adapts credits, e.g. based on free memory,
loss rate etc, in <A HREF="http://jira.jboss.com/jira/browse/JGRP-2">http://jira.jboss.com/jira/browse/JGRP-2</A>.<BR><BR>Compared
to the previous tests (2007), the message rates and throughput for
UDP is now considerably higher than before, in most cases even higher
than the TCP based configuration. This is caused by performance
modifications in JGroups and by setting core.net.rmem_max to 20000000
(from 131'000). The latter change allows the UDP stack to buffer more
datagrams, triggering fewer retransmissions and therefore increasing
performance. Note that <A HREF="https://jira.jboss.org/jira/browse/JGRP-806">https://jira.jboss.org/jira/browse/JGRP-806</A>
certainly also made a big difference.</SPAN></P>
<P LANG="en">Note that for this test, for administrative reasons, we
could not make use of jumbo frames and the MTU was 1500 instead of
9000 for the 2007 test. This likely affects throughput for larger
messages.<BR><BR>The throughput of UDP has little variance for 1K
messages and ranges from 56 MB/sec on 4 nodes to 51 MB/sec on 10
nodes. This is significantly better than TCP which starts at 49
MB/sec on 4 nodes and drops off to 34 MB/sec on 10 nodes. We can also
see that the drop in throughput for UDP is worse for 2.5K messages
(from 94 MB/sec on 4 nodes down to 65 MB/sec on 10 nodes) and for 5K
messages (from 107 MB/sec on 4 nodes down to 77 MB/sec on 10 nodes).
We attribute this to the fact that jumbo frames were not used: large
UDP datagrams are fragmented into more IP packets (that need to get
retransmitted on a single loss) with an MTU of 1500 than with 9000.
TCP doesn't have this issue as it generates IP packets which are
under the MTU's size.<BR><BR><BR>
</P>
<H3>Scalability</H3>
<P>As can be seen in “Scalability per node”, message rates and
throughput degrade slightly as we increase the cluster size.   
Reasons could be  that the switch starts dropping packets when
overloaded or flow control (FC) has too few credits (which are
statically set, but should be allocated as a function of the cluster
size). This will be investigated in one of the next releases. 
</P>
<P>Note that all numbers for message rates and throughput are shown
<I>per node</I>, ie. the number of MBs a single node receives on
average per second. If we aggregate those numbers and show the total
traffic over the entire cluster, we can see (in “Scalability per
cluster”) that we scale pretty well, although not 100% linearly.
The figure shows that we can deliver ca. 500'000 1K messages over a
cluster of 10 with UDP, and have an aggregate cluster wide throughput
of over 800 MB/sec.</P>
<H3 LANG="en">Processing of messages and scalability</H3>
<P><SPAN LANG="en">The <A HREF="http://wiki.jboss.org/wiki/Wiki.jsp?page=PerfTests">test
program</A> currently doesn't do any processing at the receiver when
a message is received. Instead, a message counter is increased and
the size added to the total number of messages received so far. This
is of course an ideal situation, and not realistic for normal use,
where receivers might unmarshal the message's byte buffer into
something meaningful (e.g. an HTTP session modification), and process
it (e.g. updating an HTTP session). (The test by the way allows to
take processing time at the receiver into account by setting the
processing_delay value). <BR><BR></SPAN><BR><BR>
</P>
<H3 LANG="en">Fluctuation of results (reproducability)</H3>
<P><SPAN LANG="en">Compared to the <A HREF="http://www.jgroups.org/javagroupsnew/perf/Report.html">2.4
performance tests</A>, we didn't run the tests multiple times.
Instead, we ran each test only once, so the numbers shown are not
averaged over multiple runs of a given test. We also didn't adjust
the JGroups configs for the different cluster sizes. For example, the
number of credits in flow control (FC and SFC) should have been
increased with increasing cluster size. However, we wanted to see
whether we get </SPAN><SPAN LANG="en"><I>reasonable numbers for the
same configs run on different clusters</I></SPAN><SPAN LANG="en">.
<BR><BR>If you run the perf tests yourself and get even better
numbers, we'd be interested to hear from you !</SPAN></P>
<H2 LANG="en">Conclusion and outlook</H2>
<P LANG="en">We showed that performance of JGroups is good for
clusters ranging from 4 to 10 nodes. Compared to the 2007 tests, the
UDP stack now has better performance than the TCP stack.<BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR>
</P>
</BODY>
</HTML>